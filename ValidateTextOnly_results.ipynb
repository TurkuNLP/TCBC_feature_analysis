{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2553f4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tenojo/miniconda3/envs/Test/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "from scripts import corpusMLfunctions as cmf\n",
    "from datasets import logging, disable_progress_bars\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "import json\n",
    "import sys\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "#Constants\n",
    "BASE_BEG = \"SnippetDatasets/\"\n",
    "BASE_MID = \"sniplen_\"\n",
    "BASE_END = \".jsonl\"\n",
    "KEYLISTS = \"Keylists.jsonl\"\n",
    "SNIPPET_LENS = ['5','10','25','50','75','100']\n",
    "CHOSEN_PARAMS = [{'c':18.677891780920422, 'tol':1e-05}]\n",
    "#Set logging to not be as annoying\n",
    "logging.set_verbosity(40)\n",
    "\n",
    "#Defining functions for the program\n",
    "def do_nothing(ex):\n",
    "    return ex\n",
    "\n",
    "def whitespace_tokenizer(ex):\n",
    "    return ex.split(\" \")\n",
    "\n",
    "#Version for only using TfIdfVectorizer with raw text as input\n",
    "def manualStudy(params, SNIPPET_LENS, keylists, i, k, cache_dir, overwrite: bool=True):\n",
    "    disable_progress_bars()\n",
    "    filename = \"TestResults/ParamOptim_List_\"+str(i)+\"_SnipLen_\"+str(SNIPPET_LENS[k])+\"_Results.jsonl\"\n",
    "    cache_file_train = cache_dir+str(i)+\"_text_\"+str(SNIPPET_LENS[k])+\"_train.jsonl\"\n",
    "    cache_file_test = cache_dir+str(i)+\"_text_\"+str(SNIPPET_LENS[k])+\"_test.jsonl\"\n",
    "    if overwrite or not os.path.exists(filename):\n",
    "        hf_cache_dir = cache_dir+str(i)+\"_text_\"+str(SNIPPET_LENS[k])+\"_ds\"\n",
    "        train_keys = keylists[i]['train_keys']\n",
    "        #Temporary edit to test with combining eval+test as we are not param optimizing\n",
    "        eval_keys = keylists[i]['eval_keys']+keylists[i]['test_keys']\n",
    "        train_dss = cmf.combineSnippedBooksToDS(train_keys, SNIPPET_LENS[k], hf_cache_dir, cache_file_train, inc_raw_text=True, folder=BASE_BEG)\n",
    "        eval_dss = cmf.combineSnippedBooksToDS(eval_keys, SNIPPET_LENS[k], hf_cache_dir,  cache_file_test, inc_raw_text=True, folder=BASE_BEG)\n",
    "        #Empty cache after we don't need it\n",
    "        os.remove(cache_file_train)\n",
    "        os.remove(cache_file_test)\n",
    "        #with open(cache_file, 'w') as writer:\n",
    "        #    writer.write(\"\")\n",
    "        #Continue on\n",
    "        vectorizer = TfidfVectorizer(norm='l2', tokenizer=whitespace_tokenizer, preprocessor=do_nothing, max_features=2000).fit(train_dss['raw_text'])\n",
    "        #print(\"Worker for length \",SNIPPET_LENS[k],\" and keylist \",i,\" activated!\")\n",
    "        returnable = []\n",
    "        for pair in params:\n",
    "            #Train a new classifier for each set of params\n",
    "            \n",
    "                clf = LinearSVC(\n",
    "                    loss='squared_hinge', penalty='l2',\n",
    "                    random_state=42,\n",
    "                    C=pair['c'],\n",
    "                    tol=pair['tol']\n",
    "                )\n",
    "                clf.fit(vectorizer.transform(train_dss['raw_text']), train_dss['label'])\n",
    "                predicted = clf.predict(vectorizer.transform(eval_dss['raw_text']))\n",
    "                f1 = f1_score(eval_dss['label'], predicted, average=\"macro\")\n",
    "                #Reverse the dictionary\n",
    "                index2feature = {}\n",
    "                for feature,idx in vectorizer.vocabulary_.items():\n",
    "                    assert idx not in index2feature #This really should hold\n",
    "                    index2feature[idx]=feature\n",
    "                #Now we can query index2feature to get the feature names as we need\n",
    "                high_prio = {}\n",
    "                # make a list of (weight, index), sort it\n",
    "                for j in clf.classes_:\n",
    "                    lst=[]\n",
    "                    for idx,weight in enumerate(clf.coef_[list(clf.classes_).index(j)]):\n",
    "                        lst.append((weight,idx))\n",
    "                    lst.sort() #sort\n",
    "\n",
    "                    #Print first few and last few\n",
    "                    #for weight,idx in lst[:20]: #first 30 (ie lowest weight)\n",
    "                    #    print(index2feature[idx])\n",
    "                    #print(\"----------------------------------------------------\")\n",
    "                    #Take the last 30 (lst[-30:]) but these now come from weakest to strongest\n",
    "                    #so reverse the list using [::-1]\n",
    "                    highest_prio = []\n",
    "                    for weight,idx in lst[-100:][::-1]:\n",
    "                        highest_prio.append(index2feature[idx])\n",
    "                    high_prio[j] = highest_prio\n",
    "                returnable.append({'keylist_id':i, 'sniplen':SNIPPET_LENS[k], 'c':pair['c'], 'tol':pair['tol'], 'f1':f1, 'important_feats_7-8':high_prio['7-8'], 'important_feats_9-12':high_prio['9-12'], 'important_feats_13+':high_prio['13+']})\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write('\\n'.join(map(json.dumps, returnable)))\n",
    "        #Clear hf cache to manage space\n",
    "        shutil.rmtree(hf_cache_dir)\n",
    "\n",
    "def testParamResults(permutations: int, keylists: list):\n",
    "    #For local machines\n",
    "    pool = mp.Pool(mp.cpu_count()-2)\n",
    "    #For CSC environments\n",
    "    #pool = mp.Pool(len(os.sched_getaffinity(0)))\n",
    "    pbar = tqdm(total=permutations*len(SNIPPET_LENS))\n",
    "    def update(*a):\n",
    "     pbar.update(1)\n",
    "    #Generate temporary cache dir to manage memory\n",
    "    cache_dir = \"cache_dir/\"\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.mkdir(cache_dir)\n",
    "    #Add to list the test results of our 'manual' study\n",
    "    for k in range(len(SNIPPET_LENS)):\n",
    "        pool.apply_async(manualStudy, [CHOSEN_PARAMS, SNIPPET_LENS, keylists, 93, k, cache_dir, True], callback=update)\n",
    "    #print(\"All running!\")\n",
    "    pool.close()\n",
    "    #print(\"Pool closed!\")\n",
    "    pool.join()\n",
    "    #print(\"Waiting done!\")\n",
    "    \n",
    "    \n",
    "\n",
    "#Main function\n",
    "def main(cmd_args):\n",
    "    #Fetch keylists\n",
    "    keylists = []\n",
    "    with open(KEYLISTS, 'r') as f:\n",
    "        for line in f:\n",
    "            keylists.append(json.loads(line))\n",
    "    #testParamResults(1, keylists)\n",
    "#Pass cmd args to main function\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59cba5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn import metrics\n",
    "from scripts import corpusMLfunctions as cmf\n",
    "from datasets import logging, disable_progress_bars\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "import json\n",
    "import sys\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "keylist_num = 93\n",
    "k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b7d4c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "keylists = []\n",
    "with open(KEYLISTS, 'r') as f:\n",
    "    for line in f:\n",
    "        keylists.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "22ac93d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining functions for the program\n",
    "def do_nothing(ex):\n",
    "    return ex.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f28d40e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskPropnWithMask(example):\n",
    "    df = cmf.snippetConllu2DF(example['conllu'])\n",
    "    df.loc[df['upos'] == 'PROPN', 'lemma'] = \"[MASK]\"\n",
    "    df.loc[df['upos'] == 'PROPN', 'text'] = \"[MASK]\"\n",
    "    example['masked_text'] = ' '.join(df['text'].to_numpy('str'))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee892fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from scripts import bookdatafunctions as bdf\n",
    "def combineSnippedBooksToDS(keys: list[str], snip_len: str, cache_dir: str, cache_file:str, inc_raw_text: bool=False, inc_conllu: bool=False, inc_hpfv: bool=False, folder:str=None):\n",
    "    #logging.set_verbosity(40)\n",
    "    #Helper function to parse json-lines\n",
    "    def jsonlReader(key: str):\n",
    "        with open(folder+key+\"/sniplen_\"+snip_len+\".jsonl\") as reader:\n",
    "            with open(cache_file, 'a') as tt:\n",
    "                 #Only include the information we need for our specific purposes to save up on cache space\n",
    "                 for line in reader:\n",
    "                        if not inc_raw_text:\n",
    "                            line = line[:line.find(\",\\\"raw_text\\\":\")] + line[line.find(\",\\\"conllu\\\":\"):]\n",
    "                        if not inc_conllu:\n",
    "                            line = line[:line.find(\",\\\"conllu\\\":\")] + line[line.find(\",\\\"hp_fv\\\":\"):]\n",
    "                        if not inc_hpfv:\n",
    "                            line = line[:line.find(\",\\\"hp_fv\\\":\")] + line[line.find(\"}\\n\"):]\n",
    "                        tt.write(line)\n",
    "    #Generate list of dicts, where each dict is a json-line\n",
    "    for k in range(len(keys)):\n",
    "        if int(bdf.findAgeFromID(keys[k])) < 9 or int(bdf.findAgeFromID(keys[k])) > 12:\n",
    "            jsonlReader(keys[k])\n",
    "    #Return a shuffled dataset\n",
    "    return Dataset.from_json(cache_file, cache_dir=cache_dir).shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22b56e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also makes it easier to clean cache files and use space more efficiently\n",
    "shutil.rmtree(\"cache_dir/temp/\")\n",
    "os.mkdir(\"cache_dir/temp/\")\n",
    "cache_dir = \"cache_dir/temp/\"\n",
    "cache_file_train = cache_dir+str(keylist_num)+\"_\"+str(SNIPPET_LENS[k])+\"_train.jsonl\"\n",
    "cache_file_eval = cache_dir+str(keylist_num)+\"_\"+str(SNIPPET_LENS[k])+\"_eval.jsonl\"\n",
    "cache_file_test = cache_dir+str(keylist_num)+\"_\"+str(SNIPPET_LENS[k])+\"_test.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c25a0baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 19938 examples [00:00, 309301.05 examples/s]\n",
      "Generating train split: 6196 examples [00:00, 339480.45 examples/s]\n",
      "Generating train split: 4034 examples [00:00, 386464.96 examples/s]\n",
      "/home/tenojo/miniconda3/envs/Test/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_keys = keylists[keylist_num]['train_keys']\n",
    "#Temporary edit to test with combining eval+test as we are not param optimizing\n",
    "eval_keys = keylists[keylist_num]['eval_keys']\n",
    "test_keys = keylists[keylist_num]['test_keys']\n",
    "train_dss = combineSnippedBooksToDS(train_keys, SNIPPET_LENS[k], cache_dir, cache_file_train, inc_raw_text=True, folder=BASE_BEG)\n",
    "eval_dss = combineSnippedBooksToDS(eval_keys, SNIPPET_LENS[k], cache_dir,  cache_file_eval, inc_raw_text=True, folder=BASE_BEG)\n",
    "test_dss = combineSnippedBooksToDS(test_keys, SNIPPET_LENS[k], cache_dir,  cache_file_test, inc_raw_text=True, folder=BASE_BEG)\n",
    "\n",
    "#train_dss = train_dss.map(maskPropnWithMask)\n",
    "#eval_dss = eval_dss.map(maskPropnWithMask)\n",
    "#test_dss = test_dss.map(maskPropnWithMask)\n",
    "\n",
    "#train_dss = train_dss.filter(lambda x: x['label'] != '9-12')\n",
    "#eval_dss = eval_dss.filter(lambda x: x['label'] != '9-12')\n",
    "#test_dss = test_dss.filter(lambda x: x['label'] != '9-12')\n",
    "#Empty cache after we don't need it\n",
    "os.remove(cache_file_train)\n",
    "os.remove(cache_file_eval)\n",
    "os.remove(cache_file_test)\n",
    "#with open(cache_file, 'w') as writer:\n",
    "#    writer.write(\"\")\n",
    "#Continue on\n",
    "vectorizer = TfidfVectorizer(norm='l2', tokenizer=whitespace_tokenizer, preprocessor=do_nothing, max_features=2000).fit(train_dss['raw_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ddeaeef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['book_id', 'age', 'label', 'raw_text'],\n",
      "    num_rows: 19938\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "pprint(train_dss)\n",
    "\n",
    "#\"Equal\" the number based on age groups\n",
    "min_nums_train = []\n",
    "min_nums_eval = []\n",
    "min_nums_test = []\n",
    "\n",
    "for i in range(1):\n",
    "    min_nums_train.append(np.min(list(Counter(train_dss['label']).values())))\n",
    "    min_nums_eval.append(np.min(list(Counter(eval_dss['label']).values())))\n",
    "    min_nums_test.append(np.min(list(Counter(test_dss['label']).values())))\n",
    "\n",
    "for i in range(1):\n",
    "    train_df = train_dss.to_pandas()\n",
    "    #sampled = pd.concat([train_df[train_df['label'] == '7-8'].sample(min_nums_train[i], replace=False), train_df[train_df['label'] == '9-12'].sample(min_nums_train[i], replace=False), train_df[train_df['label'] == '13+'].sample(min_nums_train[i], replace=False)])\n",
    "    sampled = pd.concat([train_df[train_df['label'] == '7-8'].sample(min_nums_train[i], replace=False), train_df[train_df['label'] == '13+'].sample(min_nums_train[i], replace=False)])\n",
    "    sampled = sampled[sampled['book_id'].apply(lambda x: x[-1] == '1')]\n",
    "    train_dss = Dataset.from_pandas(sampled)\n",
    "    eval_df = eval_dss.to_pandas()\n",
    "    #sampled = pd.concat([eval_df[eval_df['label'] == '7-8'].sample(min_nums_eval[i], replace=False), eval_df[eval_df['label'] == '9-12'].sample(min_nums_eval[i], replace=False), eval_df[eval_df['label'] == '13+'].sample(min_nums_eval[i], replace=False)])\n",
    "    sampled = eval_df[eval_df['book_id'].apply(lambda x: x[-1] == '1')]\n",
    "    eval_dss = Dataset.from_pandas(sampled)\n",
    "    test_df = test_dss.to_pandas()\n",
    "    #sampled = pd.concat([test_df[test_df['label'] == '7-8'].sample(min_nums_test[i], replace=False), test_df[test_df['label'] == '9-12'].sample(min_nums_test[i], replace=False), test_df[test_df['label'] == '13+'].sample(min_nums_test[i], replace=False)])\n",
    "    sampled = test_df[test_df['book_id'].apply(lambda x: x[-1] == '1')]\n",
    "    test_dss = Dataset.from_pandas(sampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44f73ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    #Defining hyperparameters to tune\n",
    "    c = trial.suggest_float('c', 1e-5, 1e+1, log=True)\n",
    "    tol = trial.suggest_categorical('tol', [1e-6, 1e-5, 1e-4])\n",
    "    clf = LinearSVC(\n",
    "        loss='squared_hinge', penalty='l2',\n",
    "        random_state=42,\n",
    "        C=c,\n",
    "        tol=tol\n",
    "    )\n",
    "    clf.fit(vectorizer.transform(train_dss['raw_text']), train_dss['label'])\n",
    "    predicted = clf.predict(vectorizer.transform(eval_dss['raw_text']))\n",
    "    return f1_score(eval_dss['label'], predicted, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "942b25be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-22 11:01:27,635] A new study created in memory with name: no-name-ad0d1ed8-7726-4da7-a33e-e35393f815a7\n",
      "[I 2025-08-22 11:01:28,955] Trial 0 finished with value: 0.7623857306318145 and parameters: {'c': 6.6205772990868486, 'tol': 1e-06}. Best is trial 0 with value: 0.7623857306318145.\n",
      "[I 2025-08-22 11:01:30,040] Trial 1 finished with value: 0.506185018090783 and parameters: {'c': 0.0003707799527256626, 'tol': 0.0001}. Best is trial 0 with value: 0.7623857306318145.\n",
      "[I 2025-08-22 11:01:31,103] Trial 2 finished with value: 0.49718619477634807 and parameters: {'c': 0.0002569563847504035, 'tol': 1e-06}. Best is trial 0 with value: 0.7623857306318145.\n",
      "[I 2025-08-22 11:01:32,305] Trial 3 finished with value: 0.7777848464844872 and parameters: {'c': 1.208654402777989, 'tol': 1e-06}. Best is trial 3 with value: 0.7777848464844872.\n",
      "[I 2025-08-22 11:01:33,442] Trial 4 finished with value: 0.7727917024072777 and parameters: {'c': 1.7442047619639292, 'tol': 0.0001}. Best is trial 3 with value: 0.7777848464844872.\n",
      "[I 2025-08-22 11:01:34,515] Trial 5 finished with value: 0.6326132924904504 and parameters: {'c': 0.002155156202692189, 'tol': 1e-06}. Best is trial 3 with value: 0.7777848464844872.\n",
      "[I 2025-08-22 11:01:35,571] Trial 6 finished with value: 0.49598033545401965 and parameters: {'c': 0.00018918695488236463, 'tol': 0.0001}. Best is trial 3 with value: 0.7777848464844872.\n",
      "[I 2025-08-22 11:01:36,637] Trial 7 finished with value: 0.757092811345968 and parameters: {'c': 0.0075668664580601725, 'tol': 0.0001}. Best is trial 3 with value: 0.7777848464844872.\n",
      "[I 2025-08-22 11:01:37,844] Trial 8 finished with value: 0.7727917024072777 and parameters: {'c': 1.6885523706937091, 'tol': 1e-06}. Best is trial 3 with value: 0.7777848464844872.\n",
      "[I 2025-08-22 11:01:38,874] Trial 9 finished with value: 0.5301606440554647 and parameters: {'c': 5.9888161854936964e-05, 'tol': 1e-05}. Best is trial 3 with value: 0.7777848464844872.\n"
     ]
    }
   ],
   "source": [
    "# Your code for hyperparameter optimization here\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3198462a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         13+       0.77      0.95      0.85      2586\n",
      "         7-8       0.78      0.36      0.50      1172\n",
      "\n",
      "    accuracy                           0.77      3758\n",
      "   macro avg       0.77      0.66      0.67      3758\n",
      "weighted avg       0.77      0.77      0.74      3758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC(\n",
    "        loss='squared_hinge', penalty='l2',\n",
    "        random_state=42,\n",
    "        C=study.best_params['c'],\n",
    "        tol=study.best_params['tol']\n",
    ")\n",
    "clf.fit(vectorizer.transform(train_dss['raw_text']), train_dss['label'])\n",
    "predicted = clf.predict(vectorizer.transform(test_dss['raw_text']))\n",
    "print(metrics.classification_report(predicted, test_dss['label']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
