{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53156104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from scripts import bookdatafunctions as bdf\n",
    "from scripts import corpusMLfunctions as cmf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict, logging\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import optuna\n",
    "import json\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59d01a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_BEG = \"SnippetDatasets/\"\n",
    "BASE_MID = \"sniplen_\"\n",
    "BASE_END = \".jsonl\"\n",
    "KEYLISTS = \"Keylists.jsonl\"\n",
    "SNIPPET_LENS = ['5','10','25','50','75','100']\n",
    "CHOSEN_PARAMS = [{'c':0.15, 'tol':1e-6}, {'c':0.4, 'tol':1e-4}, {'c':5, 'tol':1e-4}, {'c':15, 'tol':1e-3}, {'c':60, 'tol':1e-3}, {'c':120, 'tol':1e-3}]\n",
    "logging.set_verbosity(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81bdd126",
   "metadata": {},
   "outputs": [],
   "source": [
    "keylists = []\n",
    "with open(KEYLISTS, 'r') as f:\n",
    "    for line in f:\n",
    "        keylists.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbc8b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_nothing(ex):\n",
    "    return ex\n",
    "\n",
    "def conllu_tokenizer(ex):\n",
    "    return ex.replace(\"\\n\", \"\\t\").replace(\"|\", \"\\t\").split(\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888dc510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manualStudy(params, SNIPPET_LENS, keylists, i, k, overwrite: bool=True):\n",
    "    filename = \"TestResults/ParamOptim_List_\"+str(i)+\"_SnipLen_\"+str(SNIPPET_LENS[k])+\"_Results.jsonl\"\n",
    "    if overwrite or not os.path.exists(filename):\n",
    "        train_keys = keylists[i]['train_keys']\n",
    "        eval_keys = keylists[i]['eval_keys']\n",
    "        train_dss = cmf.combineSnippedBooksToDS(train_keys, SNIPPET_LENS[k], BASE_BEG)\n",
    "        eval_dss = cmf.combineSnippedBooksToDS(eval_keys, SNIPPET_LENS[k], BASE_BEG)\n",
    "        vectorizer = TfidfVectorizer(norm='l2', tokenizer=conllu_tokenizer, preprocessor=do_nothing, max_features=2000).fit(train_dss['conllu'])\n",
    "\n",
    "        vecd_train_data = vectorizer.transform(train_dss['conllu'])\n",
    "        vecd_eval_data = vectorizer.transform(eval_dss['conllu'])\n",
    "        #print(\"Worker for length \",SNIPPET_LENS[k],\" and keylist \",i,\" activated!\")\n",
    "        returnable = []\n",
    "        for pair in params:\n",
    "            #Train a new classifier for each set of params\n",
    "            \n",
    "                clf = LinearSVC(\n",
    "                    random_state=42,\n",
    "                    C=pair['c'],\n",
    "                    tol=pair['tol']\n",
    "                )\n",
    "                clf.fit(vecd_train_data, train_dss['label'])\n",
    "                predicted = clf.predict(vecd_eval_data)\n",
    "                f1 = f1_score(eval_dss['label'], predicted, average=\"macro\")\n",
    "                #Reverse the dictionary\n",
    "                index2feature = {}\n",
    "                for feature,idx in vectorizer.vocabulary_.items():\n",
    "                    assert idx not in index2feature #This really should hold\n",
    "                    index2feature[idx]=feature\n",
    "                #Now we can query index2feature to get the feature names as we need\n",
    "                high_prio = {}\n",
    "                # make a list of (weight, index), sort it\n",
    "                for j in range(3):\n",
    "                    lst=[]\n",
    "                    for idx,weight in enumerate(clf.coef_[j]):\n",
    "                        lst.append((weight,idx))\n",
    "                    lst.sort() #sort\n",
    "\n",
    "                    #Print first few and last few\n",
    "                    #for weight,idx in lst[:20]: #first 30 (ie lowest weight)\n",
    "                    #    print(index2feature[idx])\n",
    "                    #print(\"----------------------------------------------------\")\n",
    "                    #Take the last 30 (lst[-30:]) but these now come from weakest to strongest\n",
    "                    #so reverse the list using [::-1]\n",
    "                    highest_prio = []\n",
    "                    for weight,idx in lst[-100:][::-1]:\n",
    "                        highest_prio.append(index2feature[idx])\n",
    "                    high_prio[j] = highest_prio\n",
    "                returnable.append({'keylist_id':i, 'sniplen':SNIPPET_LENS[k], 'c':pair['c'], 'tol':pair['tol'], 'f1':f1, 'important_feats_7-8':high_prio[0], 'important_feats_9-12':high_prio[1], 'important_feats_13+':high_prio[2]})\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write('\\n'.join(map(json.dumps, returnable)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5de63278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testParamResults(permutations: int, keylists: list):\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    pbar = tqdm(total=permutations)\n",
    "    def update(*a):\n",
    "     pbar.update()\n",
    "    for i in range(permutations):\n",
    "        #Add to list the test results of our 'manual' study\n",
    "        for k in range(len(SNIPPET_LENS)):\n",
    "            pool.apply_async(manualStudy, [CHOSEN_PARAMS, SNIPPET_LENS, keylists, i, k], callback=update)\n",
    "    #print(\"All running!\")\n",
    "    pool.close()\n",
    "    #print(\"Pool closed!\")\n",
    "    pool.join()\n",
    "    #print(\"Waiting done!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9020db06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tenojo/miniconda3/envs/Test/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/tenojo/miniconda3/envs/Test/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/tenojo/miniconda3/envs/Test/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/tenojo/miniconda3/envs/Test/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/tenojo/miniconda3/envs/Test/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/tenojo/miniconda3/envs/Test/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "6it [05:21, 53.63s/it]\n"
     ]
    }
   ],
   "source": [
    "testParamResults(1, keylists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ab278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testForAllSets():\n",
    "    test_results = []\n",
    "    with tqdm(range(100), desc=\"Iterating through permutations...\") as pbar:\n",
    "        for i in range(2):\n",
    "            train_keys = keylists[i]['train_keys']\n",
    "            eval_keys = keylists[i]['eval_keys']\n",
    "            test_keys = keylists[i]['train_keys']\n",
    "            train_dss = [cmf.combineSnippedBooksToDS(train_keys, x, BASE_BEG) for x in SNIPPET_LENS]\n",
    "            eval_dss = [cmf.combineSnippedBooksToDS(eval_keys, x, BASE_BEG) for x in SNIPPET_LENS]\n",
    "            test_dss = [cmf.combineSnippedBooksToDS(test_keys, x, BASE_BEG) for x in SNIPPET_LENS]\n",
    "            vectorizers = [TfidfVectorizer(norm='l2', tokenizer=conllu_tokenizer, preprocessor=do_nothing, max_features=2000).fit(x['conllu']) for x in train_dss]\n",
    "\n",
    "            vecd_train_datas = [vectorizers[i].transform(train_dss[i]['conllu']) for i in range(len(SNIPPET_LENS))]\n",
    "            vecd_eval_datas = [vectorizers[i].transform(eval_dss[i]['conllu']) for i in range(len(SNIPPET_LENS))]\n",
    "            vecd_test_datas = [vectorizers[i].transform(test_dss[i]['conllu']) for i in range(len(SNIPPET_LENS))]\n",
    "\n",
    "            classifiers = {}\n",
    "            f_scores = []\n",
    "            for j in range(len(SNIPPET_LENS)):\n",
    "                clf = LinearSVC(\n",
    "                loss='squared_hinge', penalty='l2',\n",
    "                                random_state=42,\n",
    "                                C=62.42249746377182,\n",
    "                                tol=0.001)\n",
    "                clf.fit(vecd_train_datas[j], train_dss[j]['label'])\n",
    "                classifiers[SNIPPET_LENS[j]] = clf\n",
    "                test_predict = clf.predict(vecd_test_datas[j])\n",
    "                f_scores.append(f1_score(test_dss[j]['label'], test_predict, average=\"macro\"))\n",
    "\n",
    "            #Reverse the dictionary\n",
    "            index2features = {}\n",
    "            for i in range(len(SNIPPET_LENS)):\n",
    "                index2feature = {}\n",
    "                vectorizer = vectorizers[i]\n",
    "                for feature,idx in vectorizer.vocabulary_.items():\n",
    "                    assert idx not in index2feature #This really should hold\n",
    "                    index2feature[idx]=feature\n",
    "                index2features[SNIPPET_LENS[i]] = index2feature\n",
    "            #Now we can query index2feature to get the feature names as we need\n",
    "            highest_prios = {}\n",
    "            for i in SNIPPET_LENS:\n",
    "                high_prio = {}\n",
    "                classifier = classifiers[i]\n",
    "                index2feature = index2features[i]\n",
    "                # make a list of (weight, index), sort it\n",
    "                for j in range(3):\n",
    "                    lst=[]\n",
    "                    for idx,weight in enumerate(classifier.coef_[j]):\n",
    "                        lst.append((weight,idx))\n",
    "                    lst.sort() #sort\n",
    "\n",
    "                    #Print first few and last few\n",
    "                    #for weight,idx in lst[:20]: #first 30 (ie lowest weight)\n",
    "                    #    print(index2feature[idx])\n",
    "                    #print(\"----------------------------------------------------\")\n",
    "                    #Take the last 30 (lst[-30:]) but these now come from weakest to strongest\n",
    "                    #so reverse the list using [::-1]\n",
    "                    highest_prio = []\n",
    "                    for weight,idx in lst[-100:][::-1]:\n",
    "                        highest_prio.append(index2feature[idx])\n",
    "                    high_prio[j] = high_prio\n",
    "                highest_prios[i] = high_prio\n",
    "            test_results.append({'id':i, 'f1s':f_scores, 'feats':highest_prios})\n",
    "            pbar.update(1)\n",
    "    return test_results\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4eba164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating through permutations...:   0%|          | 0/100 [00:00<?, ?it/s]/home/tenojo/miniconda3/envs/Test/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Iterating through permutations...:   1%|          | 1/100 [06:13<10:15:37, 373.11s/it]/home/tenojo/miniconda3/envs/Test/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Iterating through permutations...:   2%|▏         | 2/100 [12:09<9:55:37, 364.66s/it] \n"
     ]
    }
   ],
   "source": [
    "results = testForAllSets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d04c710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '100', 'f1s': [0.5699851227037915, 0.6552567555699155, 0.7699173971144445, 0.856666833668649, 0.8923223430926203, 0.9031749401682863], 'feats': {'5': {0: {...}, 1: {...}, 2: {...}}, '10': {0: {...}, 1: {...}, 2: {...}}, '25': {0: {...}, 1: {...}, 2: {...}}, '50': {0: {...}, 1: {...}, 2: {...}}, '75': {0: {...}, 1: {...}, 2: {...}}, '100': {0: {...}, 1: {...}, 2: {...}}}}, {'id': '100', 'f1s': [0.5776534375819785, 0.657753819893878, 0.768711852225635, 0.8520056629480202, 0.8906756306062326, 0.9006938491005654], 'feats': {'5': {0: {...}, 1: {...}, 2: {...}}, '10': {0: {...}, 1: {...}, 2: {...}}, '25': {0: {...}, 1: {...}, 2: {...}}, '50': {0: {...}, 1: {...}, 2: {...}}, '75': {0: {...}, 1: {...}, 2: {...}}, '100': {0: {...}, 1: {...}, 2: {...}}}}]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed062b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_keys = keylists[0]['train_keys']\n",
    "eval_keys = keylists[0]['eval_keys']\n",
    "test_keys = keylists[0]['train_keys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7454c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dss = [cmf.combineSnippedBooksToDS(train_keys, x, BASE_BEG) for x in SNIPPET_LENS]\n",
    "eval_dss = [cmf.combineSnippedBooksToDS(eval_keys, x, BASE_BEG) for x in SNIPPET_LENS]\n",
    "test_dss = [cmf.combineSnippedBooksToDS(test_keys, x, BASE_BEG) for x in SNIPPET_LENS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12cbbae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tenojo/miniconda3/envs/Test/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizers = [TfidfVectorizer(norm='l2', tokenizer=conllu_tokenizer, preprocessor=do_nothing, max_features=2000).fit(x['conllu']) for x in train_dss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9a1d74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecd_train_datas = [vectorizers[i].transform(train_dss[i]['conllu']) for i in range(len(SNIPPET_LENS))]\n",
    "vecd_eval_datas = [vectorizers[i].transform(eval_dss[i]['conllu']) for i in range(len(SNIPPET_LENS))]\n",
    "vecd_test_datas = [vectorizers[i].transform(test_dss[i]['conllu']) for i in range(len(SNIPPET_LENS))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1613ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import hmean\n",
    "# Your code to train the machine learning model on the training set and evaluate the performance on the validation set here\n",
    "def objective(trial):\n",
    "    #Defining hyperparameters to tune\n",
    "    c = trial.suggest_float('c', 1e-1, 1e+2, log=True)\n",
    "    tol = trial.suggest_categorical('tol', [1e-6, 1e-5, 1e-4, 1e-3])\n",
    "\n",
    "    #Scaling the c-param by num_of_samples * sqrt of train size\n",
    "    \n",
    "    #c_scaler = 1\n",
    "\n",
    "    f1s = []\n",
    "    for i in range(len(SNIPPET_LENS)):\n",
    "        #c_scaler = float(len(train_dss[i]['conllu']) * np.sqrt(0.7)) \n",
    "        clf = LinearSVC(\n",
    "            random_state=42,\n",
    "            C=c,\n",
    "            tol=tol\n",
    "        )\n",
    "        clf.fit(vecd_train_datas[i], train_dss[i]['label'])\n",
    "        predicted = clf.predict(vecd_eval_datas[i])\n",
    "        f1s.append(f1_score(eval_dss[i]['label'], predicted, average=\"macro\"))\n",
    "    return np.min(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f7e92fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.linear_model import LogisticRegression\\n\\n# Your code to train the machine learning model on the training set and evaluate the performance on the validation set here\\ndef objective(trial):\\n    #Defining hyperparameters to tune\\n    c = trial.suggest_float(\\'c\\', 1e-5, 1e+1, log=True)\\n    tol = trial.suggest_categorical(\\'tol\\', [1e-6, 1e-5, 1e-4, 1e-3])\\n\\n    #Scaling the c-param by num_of_samples * sqrt of train size\\n    #c_scaler = float(len(train_ds[\\'conllu\\']) * np.sqrt(0.7)) \\n    c_scaler = 1\\n\\n    f1s = []\\n    for i in range(len(SNIPPET_LENS)):\\n        clf = LogisticRegression(\\n            penalty=\\'l1\\',\\n            solver=\\'liblinear\\',\\n            random_state=42,\\n            C=c,\\n            tol=tol\\n        )\\n        clf.fit(vecd_train_datas[i], train_dss[i][\\'age\\'])\\n        predicted = clf.predict(vecd_eval_datas[i])\\n        f1s.append(f1_score(eval_dss[i][\\'age\\'], predicted, average=\"macro\"))\\n    return hmean(f1s)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Your code to train the machine learning model on the training set and evaluate the performance on the validation set here\n",
    "def objective(trial):\n",
    "    #Defining hyperparameters to tune\n",
    "    c = trial.suggest_float('c', 1e-5, 1e+1, log=True)\n",
    "    tol = trial.suggest_categorical('tol', [1e-6, 1e-5, 1e-4, 1e-3])\n",
    "\n",
    "    #Scaling the c-param by num_of_samples * sqrt of train size\n",
    "    #c_scaler = float(len(train_ds['conllu']) * np.sqrt(0.7)) \n",
    "    c_scaler = 1\n",
    "\n",
    "    f1s = []\n",
    "    for i in range(len(SNIPPET_LENS)):\n",
    "        clf = LogisticRegression(\n",
    "            penalty='l1',\n",
    "            solver='liblinear',\n",
    "            random_state=42,\n",
    "            C=c,\n",
    "            tol=tol\n",
    "        )\n",
    "        clf.fit(vecd_train_datas[i], train_dss[i]['age'])\n",
    "        predicted = clf.predict(vecd_eval_datas[i])\n",
    "        f1s.append(f1_score(eval_dss[i]['age'], predicted, average=\"macro\"))\n",
    "    return hmean(f1s)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61d6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-19 09:21:14,301] A new study created in memory with name: no-name-ae9a8335-e3de-4ef1-9cc6-771782f02866\n",
      "[I 2025-06-19 09:21:27,434] Trial 0 finished with value: 0.19579512827637305 and parameters: {'c': 0.007783548769941197, 'tol': 1e-05}. Best is trial 0 with value: 0.19579512827637305.\n",
      "[I 2025-06-19 09:21:45,665] Trial 1 finished with value: 0.26692117161017953 and parameters: {'c': 0.0583549361905685, 'tol': 0.0001}. Best is trial 1 with value: 0.26692117161017953.\n",
      "[I 2025-06-19 09:21:50,760] Trial 2 finished with value: 0.19579512827637305 and parameters: {'c': 0.00022525489149142507, 'tol': 0.0001}. Best is trial 1 with value: 0.26692117161017953.\n",
      "[I 2025-06-19 09:23:49,597] Trial 3 finished with value: 0.3788184883974371 and parameters: {'c': 25.54808299451608, 'tol': 1e-05}. Best is trial 3 with value: 0.3788184883974371.\n",
      "[I 2025-06-19 09:23:53,822] Trial 4 finished with value: 0.19579512827637305 and parameters: {'c': 3.87742306223791e-05, 'tol': 0.0001}. Best is trial 3 with value: 0.3788184883974371.\n",
      "[I 2025-06-19 09:25:47,680] Trial 5 finished with value: 0.3786912903278717 and parameters: {'c': 23.32126569967973, 'tol': 1e-05}. Best is trial 3 with value: 0.3788184883974371.\n",
      "[I 2025-06-19 09:25:51,417] Trial 6 finished with value: 0.19579512827637305 and parameters: {'c': 2.650221652163786e-05, 'tol': 0.0001}. Best is trial 3 with value: 0.3788184883974371.\n",
      "[I 2025-06-19 09:29:05,319] Trial 7 finished with value: 0.37904597781105903 and parameters: {'c': 50.82799018853293, 'tol': 1e-06}. Best is trial 7 with value: 0.37904597781105903.\n",
      "[I 2025-06-19 09:29:26,957] Trial 8 finished with value: 0.2419249399035368 and parameters: {'c': 0.031547938788716055, 'tol': 1e-06}. Best is trial 7 with value: 0.37904597781105903.\n",
      "[I 2025-06-19 09:29:58,463] Trial 9 finished with value: 0.28252690847039924 and parameters: {'c': 0.12517431665028053, 'tol': 1e-06}. Best is trial 7 with value: 0.37904597781105903.\n",
      "[I 2025-06-19 09:30:28,149] Trial 10 finished with value: 0.35323941683856847 and parameters: {'c': 1.2053518361382352, 'tol': 0.001}. Best is trial 7 with value: 0.37904597781105903.\n",
      "[I 2025-06-19 09:34:04,292] Trial 11 finished with value: 0.37925690458683287 and parameters: {'c': 73.15492708047228, 'tol': 1e-06}. Best is trial 11 with value: 0.37925690458683287.\n",
      "[I 2025-06-19 09:37:53,270] Trial 12 finished with value: 0.3792831235308381 and parameters: {'c': 83.86890117223037, 'tol': 1e-06}. Best is trial 12 with value: 0.3792831235308381.\n",
      "[I 2025-06-19 09:38:59,509] Trial 13 finished with value: 0.3591733998586601 and parameters: {'c': 1.4339747754036303, 'tol': 1e-06}. Best is trial 12 with value: 0.3792831235308381.\n",
      "[I 2025-06-19 09:40:19,583] Trial 14 finished with value: 0.3765649060940753 and parameters: {'c': 2.9099573553551576, 'tol': 1e-06}. Best is trial 12 with value: 0.3792831235308381.\n",
      "[I 2025-06-19 09:41:13,356] Trial 15 finished with value: 0.3793497189025488 and parameters: {'c': 62.42249746377182, 'tol': 0.001}. Best is trial 15 with value: 0.3793497189025488.\n",
      "[I 2025-06-19 09:41:53,105] Trial 16 finished with value: 0.37750488263597487 and parameters: {'c': 6.179849087976635, 'tol': 0.001}. Best is trial 15 with value: 0.3793497189025488.\n",
      "[I 2025-06-19 09:42:14,182] Trial 17 finished with value: 0.29886867347814866 and parameters: {'c': 0.28447525244666877, 'tol': 0.001}. Best is trial 15 with value: 0.3793497189025488.\n",
      "[I 2025-06-19 09:42:20,863] Trial 18 finished with value: 0.19579512827637305 and parameters: {'c': 0.0018764326253213046, 'tol': 0.001}. Best is trial 15 with value: 0.3793497189025488.\n",
      "[I 2025-06-19 09:43:02,459] Trial 19 finished with value: 0.3782013051879594 and parameters: {'c': 7.22995101968555, 'tol': 0.001}. Best is trial 15 with value: 0.3793497189025488.\n"
     ]
    }
   ],
   "source": [
    "# Your code for hyperparameter optimization here\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d707776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial (number 15):\n",
      "  Value: 0.3793497189025488\n",
      "  Params: {'c': 62.42249746377182, 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# Print the best trial's hyperparameters and objective value\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best trial (number {best_trial.number}):\")\n",
    "print(f\"  Value: {best_trial.value}\")\n",
    "print(f\"  Params: {best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0c4250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16cfaeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_scalers = []\n",
    "for i in range(len(SNIPPET_LENS)):\n",
    "    c_scalers.append(best_trial.params['c']*(float(len(train_dss[i]['conllu']) * np.sqrt(0.7)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "073681aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11005566.586019961, 5504193.406036358, 2203014.3584691826, 1102708.3866274317, 736183.4525859752, 552607.6271149407]\n"
     ]
    }
   ],
   "source": [
    "print(c_scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc71cbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for snippet length:  5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         13+       0.79      0.64      0.70    116490\n",
      "         7-8       0.35      0.57      0.44     24643\n",
      "        9-12       0.55      0.60      0.57     69595\n",
      "\n",
      "    accuracy                           0.62    210728\n",
      "   macro avg       0.56      0.60      0.57    210728\n",
      "weighted avg       0.66      0.62      0.63    210728\n",
      "\n",
      "Results for snippet length:  10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         13+       0.82      0.71      0.76     54520\n",
      "         7-8       0.50      0.65      0.56     15524\n",
      "        9-12       0.62      0.67      0.64     35347\n",
      "\n",
      "    accuracy                           0.69    105391\n",
      "   macro avg       0.65      0.67      0.65    105391\n",
      "weighted avg       0.70      0.69      0.69    105391\n",
      "\n",
      "Results for snippet length:  25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         13+       0.87      0.80      0.84     20555\n",
      "         7-8       0.68      0.76      0.72      7194\n",
      "        9-12       0.73      0.77      0.75     14433\n",
      "\n",
      "    accuracy                           0.79     42182\n",
      "   macro avg       0.76      0.78      0.77     42182\n",
      "weighted avg       0.79      0.79      0.79     42182\n",
      "\n",
      "Results for snippet length:  50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         13+       0.92      0.87      0.90     10001\n",
      "         7-8       0.80      0.86      0.83      3768\n",
      "        9-12       0.83      0.86      0.84      7345\n",
      "\n",
      "    accuracy                           0.86     21114\n",
      "   macro avg       0.85      0.86      0.86     21114\n",
      "weighted avg       0.87      0.86      0.87     21114\n",
      "\n",
      "Results for snippet length:  75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         13+       0.95      0.90      0.92      6609\n",
      "         7-8       0.85      0.89      0.87      2588\n",
      "        9-12       0.87      0.90      0.88      4899\n",
      "\n",
      "    accuracy                           0.90     14096\n",
      "   macro avg       0.89      0.90      0.89     14096\n",
      "weighted avg       0.90      0.90      0.90     14096\n",
      "\n",
      "Results for snippet length:  100\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         13+       0.96      0.91      0.93      4963\n",
      "         7-8       0.87      0.90      0.88      1958\n",
      "        9-12       0.88      0.91      0.89      3660\n",
      "\n",
      "    accuracy                           0.91     10581\n",
      "   macro avg       0.90      0.91      0.90     10581\n",
      "weighted avg       0.91      0.91      0.91     10581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(SNIPPET_LENS)):\n",
    "    clf = LinearSVC(\n",
    "    loss='squared_hinge', penalty='l2',\n",
    "                    random_state=42,\n",
    "                    C=62.42249746377182,\n",
    "                    tol=best_trial.params['tol'])\n",
    "    clf.fit(vecd_train_datas[i], train_dss[i]['label'])\n",
    "    classifiers[SNIPPET_LENS[i]] = clf\n",
    "    test_predict = clf.predict(vecd_test_datas[i])\n",
    "    print(\"Results for snippet length: \",SNIPPET_LENS[i])\n",
    "    print(metrics.classification_report(test_predict, test_dss[i]['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3170882",
   "metadata": {},
   "source": [
    "# Most important features comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e11cabd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reverse the dictionary\n",
    "index2features = {}\n",
    "for i in range(len(SNIPPET_LENS)):\n",
    "    index2feature = {}\n",
    "    vectorizer = vectorizers[i]\n",
    "    for feature,idx in vectorizer.vocabulary_.items():\n",
    "        assert idx not in index2feature #This really should hold\n",
    "        index2feature[idx]=feature\n",
    "    index2features[SNIPPET_LENS[i]] = index2feature\n",
    "#Now we can query index2feature to get the feature names as we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "505dc799",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_prios = {}\n",
    "for i in SNIPPET_LENS:\n",
    "    classifier = classifiers[i]\n",
    "    index2feature = index2features[i]\n",
    "    # make a list of (weight, index), sort it\n",
    "    lst=[]\n",
    "    for idx,weight in enumerate(classifier.coef_[2]):\n",
    "        lst.append((weight,idx))\n",
    "    lst.sort() #sort\n",
    "\n",
    "    #Print first few and last few\n",
    "    #for weight,idx in lst[:20]: #first 30 (ie lowest weight)\n",
    "    #    print(index2feature[idx])\n",
    "    #print(\"----------------------------------------------------\")\n",
    "    #Take the last 30 (lst[-30:]) but these now come from weakest to strongest\n",
    "    #so reverse the list using [::-1]\n",
    "    highest_prio = []\n",
    "    for weight,idx in lst[-20:][::-1]:\n",
    "        highest_prio.append(index2feature[idx])\n",
    "    highest_prios[i] = highest_prio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ef6e518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10': ['Orvokki',\n",
      "        'orvokki',\n",
      "        'leiri',\n",
      "        'soturi',\n",
      "        'klaani',\n",
      "        'astella',\n",
      "        'hevonen',\n",
      "        'jumala',\n",
      "        'pentu',\n",
      "        'ratsastaa',\n",
      "        'hotelli',\n",
      "        'lehti',\n",
      "        'mutisi',\n",
      "        'tohtori',\n",
      "        'vastaa',\n",
      "        'sanoi',\n",
      "        'päällikkö',\n",
      "        'C',\n",
      "        'huudahtaa',\n",
      "        'Sitten'],\n",
      " '100': ['leiri',\n",
      "         'orvokki',\n",
      "         'Orvokki',\n",
      "         'jumala',\n",
      "         'soturi',\n",
      "         'pian',\n",
      "         'hevonen',\n",
      "         'hotelli',\n",
      "         'luultavasti',\n",
      "         'luokse',\n",
      "         'klaani',\n",
      "         'huudahtaa',\n",
      "         'sanoi',\n",
      "         'ratsastaa',\n",
      "         'Sitten',\n",
      "         'sisko',\n",
      "         'tosiaan',\n",
      "         'teksti',\n",
      "         '!',\n",
      "         'vastaa'],\n",
      " '25': ['Orvokki',\n",
      "        'orvokki',\n",
      "        'leiri',\n",
      "        'soturi',\n",
      "        'klaani',\n",
      "        'astella',\n",
      "        'jumala',\n",
      "        'hevonen',\n",
      "        'vastaa',\n",
      "        'hotelli',\n",
      "        'huudahtaa',\n",
      "        'Pian',\n",
      "        'sanoi',\n",
      "        'ratsastaa',\n",
      "        'pentu',\n",
      "        'Sitten',\n",
      "        'luultavasti',\n",
      "        'nyökätä',\n",
      "        'Yritin',\n",
      "        'samassa'],\n",
      " '5': ['Orvokki',\n",
      "       'leiri',\n",
      "       'klaani',\n",
      "       'soturi',\n",
      "       'orvokki',\n",
      "       'hevonen',\n",
      "       'pentu',\n",
      "       'astella',\n",
      "       'jumala',\n",
      "       'lehti',\n",
      "       'päällikkö',\n",
      "       'ratsastaa',\n",
      "       'tohtori',\n",
      "       'CCONJ',\n",
      "       'C',\n",
      "       'lohi#käärme',\n",
      "       'hotelli',\n",
      "       'tassu',\n",
      "       'kasvi',\n",
      "       'sanoi'],\n",
      " '50': ['leiri',\n",
      "        'Orvokki',\n",
      "        'orvokki',\n",
      "        'soturi',\n",
      "        'klaani',\n",
      "        'astella',\n",
      "        'jumala',\n",
      "        'hotelli',\n",
      "        'huudahtaa',\n",
      "        'hevonen',\n",
      "        'luultavasti',\n",
      "        'ratsastaa',\n",
      "        'sanoi',\n",
      "        'vastaa',\n",
      "        'luokse',\n",
      "        'Sitten',\n",
      "        'pian',\n",
      "        'onnistua',\n",
      "        'Pian',\n",
      "        'samassa'],\n",
      " '75': ['leiri',\n",
      "        'orvokki',\n",
      "        'Orvokki',\n",
      "        'soturi',\n",
      "        'klaani',\n",
      "        'hevonen',\n",
      "        'jumala',\n",
      "        'luultavasti',\n",
      "        'pian',\n",
      "        'hotelli',\n",
      "        'astella',\n",
      "        'luokse',\n",
      "        'sanoi',\n",
      "        'huudahtaa',\n",
      "        'ratsastaa',\n",
      "        'Sitten',\n",
      "        'sisko',\n",
      "        'onnistua',\n",
      "        'hammas',\n",
      "        'vastaa']}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(highest_prios)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
