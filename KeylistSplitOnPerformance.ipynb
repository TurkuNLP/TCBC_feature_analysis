{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9a69a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from scripts import bookdatafunctions as bdf\n",
    "from scripts import corpusMLfunctions as cmf\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import random\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6fbd1f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "AGE_SHEET = \"ISBN_MAPS/ISBN2AGE.xlsx\"\n",
    "AUTH_SHEET = \"ISBN_MAPS/ISBN2AUTH.xlsx\"\n",
    "CONLLUS_FOLDER = \"Conllus\"\n",
    "SNIPPET_LENS = [5,10,25,50,75,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "89cd7fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load corpus\n",
    "corpus = bdf.mapGroup2Age(bdf.maskPropn(bdf.initBooksFromConllus(CONLLUS_FOLDER)), AGE_SHEET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf2a8a",
   "metadata": {},
   "source": [
    "## Test with one book in one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d88f128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAgeStratificationAmounts(corpus_with_ages: dict[str,pd.DataFrame], train_size: float) -> tuple[dict[int,int],dict[int,int],dict[int,int]]:\n",
    "    train = {}\n",
    "    test = {}\n",
    "    eval = {}\n",
    "    ages = bdf.getAvailableAges(corpus_with_ages)\n",
    "    for age in ages:\n",
    "        raw_amount = len([x for x in list(corpus_with_ages.keys()) if bdf.findAgeFromID(x)==str(age)])\n",
    "        train[age] = int(raw_amount*train_size)\n",
    "        test[age] = int((raw_amount-train[age])/2)\n",
    "        eval[age] = int((raw_amount-train[age])/2)\n",
    "    return train, test, eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4e2e3615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkGenreBalance(keylist: list[str], train_size: float, key_to_add: str):\n",
    "    fiction_size = 225*train_size\n",
    "    nonfiction_size = 45*train_size\n",
    "    textbook_size = 30*train_size\n",
    "\n",
    "    amounts = len([y for y in keylist if y[-1] == key_to_add[-1]])\n",
    "    if key_to_add[-1] == '1' and amounts < fiction_size:\n",
    "        return True\n",
    "    elif key_to_add[-1] == '2' and amounts < nonfiction_size:\n",
    "        return True\n",
    "    elif key_to_add[-1] == '3' and amounts < textbook_size:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "650998e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doTrainTestEvalSplitWithGenres(keys: list[str], train_target_amounts: dict[int,int], test_target_amounts: dict[int,int], eval_target_amounts: dict[int,int]):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function which splits a corpus into (roughly) stratified datasets for training, evaluation, and testing\n",
    "    \"\"\"\n",
    "    train_keys = []\n",
    "    test_keys = []\n",
    "    eval_keys = []\n",
    "    random.shuffle(keys)\n",
    "    for key in keys:\n",
    "        #Get dicts for age:number of entries\n",
    "        current_train = cmf.getNumOfEntriesPerAge(train_keys)\n",
    "        current_test = cmf.getNumOfEntriesPerAge(test_keys)\n",
    "        #Check which list to add the key to\n",
    "        age = int(bdf.findAgeFromID(key))\n",
    "        if current_train.get(age, -1) < train_target_amounts[age] and checkGenreBalance(train_keys, 0.7, key):\n",
    "            train_keys.append(key)\n",
    "        elif current_test.get(age, -1) < test_target_amounts[age] and checkGenreBalance(test_keys, 0.15, key):\n",
    "            test_keys.append(key)\n",
    "        else:\n",
    "            eval_keys.append(key)\n",
    "\n",
    "    return train_keys, test_keys, eval_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1614cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate keylist with all snippets from one book being in one dataset\n",
    "keys = list(corpus.keys())\n",
    "train_target, test_target, eval_target = generateAgeStratificationAmounts(corpus, 0.7)\n",
    "train_keys, test_keys, eval_keys = doTrainTestEvalSplitWithGenres(keys, train_target, test_target, eval_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "04a7bc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n",
      "44\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "print(len(train_keys))\n",
    "print(len(test_keys))\n",
    "print(len(eval_keys))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b879dee",
   "metadata": {},
   "source": [
    "## Test with only train-test-eval splitting by huggingface (snippets can be in any set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c40ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae3a4a3f",
   "metadata": {},
   "source": [
    "## Run TFIDF test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6de20b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from scripts import bookdatafunctions as bdf\n",
    "from scripts import corpusMLfunctions as cmf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, disable_progress_bars\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import optuna\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import shutil\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Constants\n",
    "AGES = ['5','6','7','8','9','10','11','12','13','14','15']\n",
    "BASE_BEG = \"SnippetDatasets/\"\n",
    "BASE_MID = \"sniplen_\"\n",
    "BASE_END = \".jsonl\"\n",
    "KEYLISTS = \"Keylists.jsonl\"\n",
    "keylists = []\n",
    "with open(KEYLISTS, 'r') as f:\n",
    "    for line in f:\n",
    "        keylists.append(json.loads(line))\n",
    "\n",
    "#Helper functions\n",
    "def do_nothing(ex):\n",
    "    return ex.lower()\n",
    "\n",
    "def conllu_tokenizer(ex):\n",
    "    return ex.replace(\"\\n\", \"\\t\").replace(\"|\", \"\\t\").split(\"\\t\")\n",
    "\n",
    "def whitespace_tokenizer(ex):\n",
    "    return ex.split(\" \")\n",
    "\n",
    "\n",
    "def generateIntervals(ages: list[str]):\n",
    "    intervals = []\n",
    "    #Powersets of 2 intervals\n",
    "    for i in range(2, len(ages)-1):\n",
    "        #temp = intervals[2]\n",
    "        intervals.append((ages[:i], ages[i:]))\n",
    "        #intervals[2] = temp\n",
    "    #Powersets for 3 intervals\n",
    "    for i in range(2, len(ages)-3):\n",
    "        for j in range(i+2, len(ages)-1):\n",
    "            #temp = intervals[3]\n",
    "            intervals.append((ages[:i], ages[i:j], ages[j:]))\n",
    "            #intervals[3] = temp\n",
    "    #Powersets for 4 intervals\n",
    "    for i in range(2, len(ages)-5):\n",
    "        for j in range(i+2, len(ages)-3):\n",
    "            for k in range(j+2, len(ages)-1):\n",
    "                #temp = intervals[4]\n",
    "                intervals.append((ages[:i], ages[i:j], ages[j:k], ages[k:]))\n",
    "                #intervals[4] = temp\n",
    "    return intervals\n",
    "\n",
    "def reMapLabels(ex, intervals):\n",
    "    age = ex['age']\n",
    "    if age > 14:\n",
    "        age = 15\n",
    "    age = str(age)\n",
    "    for n in intervals:\n",
    "        if age in n:\n",
    "            ex['label'] = n[0]+'-'+n[-1]\n",
    "    return ex\n",
    "\n",
    "def initDatasets():\n",
    "    train_keys = keylists[0]['train_keys']\n",
    "    eval_keys = keylists[0]['eval_keys']\n",
    "    test_keys = keylists[0]['test_keys']\n",
    "    #Also makes it easier to clean cache files and use space more efficiently\n",
    "    cache_dir = \"cache_dir/temp/\"\n",
    "    if os.path.exists(cache_dir):\n",
    "        shutil.rmtree(cache_dir)\n",
    "    os.mkdir(cache_dir)\n",
    "    train_ds = cmf.combineSnippedBooksToDS(train_keys, '50', cache_dir, cache_file=cache_dir+\"0_50_train.jsonl\", folder=BASE_BEG, inc_raw_text=True, inc_hpfv=True)\n",
    "    eval_ds = cmf.combineSnippedBooksToDS(eval_keys, '50', cache_dir, cache_file=cache_dir+\"0_50_eval.jsonl\", folder=BASE_BEG, inc_raw_text=True, inc_hpfv=True)\n",
    "    test_ds = cmf.combineSnippedBooksToDS(test_keys, '50', cache_dir, cache_file=cache_dir+\"0_50_test.jsonl\", folder=BASE_BEG, inc_raw_text=True, inc_hpfv=True)\n",
    "    return train_ds, eval_ds, test_ds\n",
    "\n",
    "def initDatasetsNoBookLevelSplitting():\n",
    "    train_keys = keylists[0]['train_keys'] + keylists[0]['eval_keys'] + keylists[0]['test_keys']\n",
    "    #Also makes it easier to clean cache files and use space more efficiently\n",
    "    cache_dir = \"cache_dir/temp/\"\n",
    "    if os.path.exists(cache_dir):\n",
    "        shutil.rmtree(cache_dir)\n",
    "    os.mkdir(cache_dir)\n",
    "    train_ds = cmf.combineSnippedBooksToDS(train_keys, '50', cache_dir, cache_file=cache_dir+\"0_50_train.jsonl\", folder=BASE_BEG, inc_raw_text=True, inc_hpfv=True)\n",
    "    train_ds = train_ds.class_encode_column(\"age\")\n",
    "    pprint(train_ds.features)\n",
    "    train_test = train_ds.train_test_split(test_size=0.7, stratify_by_column='age')\n",
    "    test_eval = train_test['test'].train_test_split(test_size=0.5, stratify_by_column='age')\n",
    "\n",
    "\n",
    "\n",
    "    return train_test['train'], test_eval['train'], test_eval['test']\n",
    "\n",
    "#Code to run in parallel\n",
    "def evaluateGroups(train_ds: Dataset, eval_ds: Dataset, test_ds: Dataset, intervals):\n",
    "    #Map labels to match the intervals given (aka re-assign labels)\n",
    "    train_ds = train_ds.map(reMapLabels, fn_kwargs={\"intervals\":intervals})\n",
    "    eval_ds = eval_ds.map(reMapLabels, fn_kwargs={\"intervals\":intervals})\n",
    "    test_ds = test_ds.map(reMapLabels, fn_kwargs={\"intervals\":intervals})\n",
    "    #Initialize and fir our vectorizer\n",
    "    vectorizer = TfidfVectorizer(norm='l2', tokenizer=whitespace_tokenizer, preprocessor=do_nothing, max_features=2000).fit(train_ds['raw_text'])\n",
    "    #Vectorize datasets (re-using other code.. This could be neater but oh well :)\n",
    "    vectorized_train = vectorizer.transform(train_ds['raw_text'])\n",
    "    vectorized_eval = vectorizer.transform(eval_ds['raw_text'])\n",
    "    vectorized_test = vectorizer.transform(test_ds['raw_text'])\n",
    "\n",
    "    returnable = {}\n",
    "    c_eval_pairs= []\n",
    "    \n",
    "    #Very quick hyperparam optimization as we have computational resources\n",
    "    def objective(trial):\n",
    "        #Defining hyperparameters to tune\n",
    "        c = trial.suggest_float('c', 1e-10, 1e+0, log=True)\n",
    "        pen = trial.suggest_categorical('pen', ['l1', 'l2'])\n",
    "        tol = trial.suggest_float('tol', 1e-10, 1e-3, log=True)\n",
    "        clf = LinearSVC(\n",
    "            random_state=42,\n",
    "            C=c,\n",
    "            tol=tol,\n",
    "            penalty=pen\n",
    "        )\n",
    "        clf.fit(vectorized_train, train_ds['label'])\n",
    "        predicted = clf.predict(vectorized_eval)\n",
    "        f1 = f1_score(eval_ds['label'], predicted, average=\"macro\")\n",
    "        c_eval_pairs.append([c, f1])\n",
    "        return f1\n",
    "\n",
    "    # Your code for hyperparameter optimization here\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    optuna.logging.disable_default_handler()\n",
    "    study.optimize(objective, n_trials=25)\n",
    "\n",
    "    #Run with best params\n",
    "    clf = LinearSVC(\n",
    "        penalty=study.best_trial.params['pen'],\n",
    "        random_state=42,\n",
    "        C=study.best_trial.params['c'],\n",
    "        tol=study.best_trial.params['tol'],\n",
    "    )\n",
    "    clf.fit(vectorized_train, train_ds['label'])\n",
    "    test_predict = clf.predict(vectorized_test)\n",
    "\n",
    "    #Generate unique id\n",
    "    id = \"_\".join([x[0]+\"-\"+x[-1] for x in intervals])\n",
    "\n",
    "    #Assign returnble values\n",
    "    returnable['f1'] = f1_score(test_ds['label'], test_predict, average=\"macro\")\n",
    "    returnable['id'] = id\n",
    "    returnable['labels'] = clf.classes_.tolist()\n",
    "    returnable['conf_matrix'] = metrics.confusion_matrix(test_ds['label'], test_predict).tolist()\n",
    "    returnable['c'] = study.best_trial.params['c']\n",
    "    returnable['tol'] = study.best_trial.params['tol']\n",
    "    returnable['penalty'] = study.best_trial.params['pen']\n",
    "    #returnable['c_eval_scores'] = c_eval_pairs\n",
    "\n",
    "    #Write JSON-file\n",
    "    #with open(\"TestResults/DifferentAgeGroups_tfidf/\"+id+\".json\", 'w') as f:\n",
    "    #    f.write(json.dumps(returnable))\n",
    "\n",
    "    return returnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "84286135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:25<00:00, 85.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore') \n",
    "os.environ['PYTHONWARNINGS']='ignore'\n",
    "disable_progress_bars()\n",
    "train_ds_base, eval_ds_base, test_ds_base = initDatasets()\n",
    "interval_splits = generateIntervals(AGES)[21:22]\n",
    "dicts = []\n",
    "with tqdm(total=len(interval_splits)) as pbar:\n",
    "    for i in interval_splits:\n",
    "        dicts.append(evaluateGroups(train_ds_base, eval_ds_base, test_ds_base, i))\n",
    "        pbar.update(1)\n",
    "print(\"Waiting done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e73c76ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'c': 0.12994152259537403,\n",
      "  'conf_matrix': [[893, 88, 342], [118, 393, 220], [470, 268, 247]],\n",
      "  'f1': 0.4811302060604299,\n",
      "  'id': '5-8_9-12_13-15',\n",
      "  'labels': ['13-15', '5-8', '9-12'],\n",
      "  'penalty': 'l2',\n",
      "  'tol': 2.4568823492943698e-08}]\n"
     ]
    }
   ],
   "source": [
    "pprint(dicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
